# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14m0Q03c1p7VEkNTxF8QalZ4uu1-YREkP
"""

import json
import re
import unicodedata

class SearchEnginePreparation:
    def __init__(self):
        self.indexes = {}
        self.synonyms = {}

    def load_indexes(self, index_files):
        """Charger tous les index à partir des fichiers JSON"""
        for file_path in index_files:
            index_name = file_path.split('/')[-1].replace('.json', '')
            with open(file_path, 'r', encoding='utf-8') as f:
                self.indexes[index_name] = json.load(f)
        print(f"Indexes chargés : {list(self.indexes.keys())}")

    def load_synonyms(self, synonyms_file):
        """Charger les synonymes à partir du fichier JSON"""
        with open(synonyms_file, 'r', encoding='utf-8') as f:
            origin_synonyms = json.load(f)

        # Créer un dictionnaire de synonymes inversé
        for primary, synonym_list in origin_synonyms.items():
            for synonym in synonym_list:
                self.synonyms[synonym.lower()] = primary.lower()

        print(f"Nombre de synonymes chargés : {len(self.synonyms)}")

    def tokenize(self, text):
        """Fonction de tokenization"""
        # Normalisation et nettoyage
        text = unicodedata.normalize('NFKD', text.lower())
        text = text.encode('ascii', 'ignore').decode('utf-8')

        # Tokenization
        tokens = re.findall(r'\b\w+\b', text)
        return tokens

    def replace_synonyms(self, tokens):
        """Remplacer les tokens par leurs synonymes si possible"""
        return [self.synonyms.get(token, token) for token in tokens]

def main():
    index_files = [
        'brand_index.json',
        'domain_index.json',
        'origin_index.json',
        'title_index.json',
        'description_index.json',
        'reviews_index.json'
    ]

    search_prep = SearchEnginePreparation()
    search_prep.load_indexes(index_files)
    search_prep.load_synonyms('origin_synonyms.json')

    # Tests de remplacement de synonymes
    test_cases = [
        "Smartphone made in United States",
        "Produit de France",
        "Korean technology",
        "Swiss precision"
    ]

    for test_text in test_cases:
        tokens = search_prep.tokenize(test_text)
        tokens_with_synonyms = search_prep.replace_synonyms(tokens)

        print("\nTexte original:", test_text)
        print("Tokens originaux:", tokens)
        print("Tokens avec synonymes:", tokens_with_synonyms)

if __name__ == "__main__":
    main()

import json
import re
import unicodedata
import nltk
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords

class SearchEngineQueryProcessor:
    def __init__(self, synonyms_file='origin_synonyms.json', stopwords_lang='french'):
        self.synonyms = self._load_synonyms(synonyms_file)
        self.stopwords = set(stopwords.words(stopwords_lang))

    def _load_synonyms(self, synonyms_file):
        """Charger les synonymes et créer un dictionnaire inversé"""
        with open(synonyms_file, 'r', encoding='utf-8') as f:
            origin_synonyms = json.load(f)

        synonyms_dict = {}
        for primary, synonym_list in origin_synonyms.items():
            for synonym in synonym_list:
                synonyms_dict[synonym.lower()] = primary.lower()
        return synonyms_dict

    def tokenize(self, text):
        """Tokenization et normalisation"""
        if not text:
            return []
        text = unicodedata.normalize('NFKD', text.lower())
        text = text.encode('ascii', 'ignore').decode('utf-8')
        return re.findall(r'\b\w+\b', text)

    def augment_query(self, tokens):
        """Augmentation de la requête par synonymes"""
        augmented_tokens = []
        for token in tokens:
            augmented_tokens.append(token)
            if token in self.synonyms:
                augmented_tokens.append(self.synonyms[token])
        return list(set(augmented_tokens))

    def filter_documents_any_token(self, documents, query_tokens):
        """Filtrer les documents avec au moins un token de la requête"""
        return [
            doc for doc in documents
            if any(token in doc['tokens'] for token in query_tokens)
        ]

    def filter_documents_all_tokens(self, documents, query_tokens):
        """Filtrer les documents avec tous les tokens (hors stopwords)"""
        filtered_tokens = [token for token in query_tokens if token not in self.stopwords]
        return [
            doc for doc in documents
            if all(token in doc['tokens'] for token in filtered_tokens)
        ]

def main():
    # Chargement des documents
    with open('products.jsonl', 'r', encoding='utf-8') as f:
        documents = [json.loads(line) for line in f]

    # Tokenization préalable des documents
    query_processor = SearchEngineQueryProcessor()
    for doc in documents:
        # Combiner tous les champs textuels disponibles
        text_to_tokenize = ' '.join(filter(None, [
            doc.get('title', ''),
            doc.get('description', ''),
            str(doc.get('product_features', '')),
            ' '.join(doc.get('links', []))
        ]))
        doc['tokens'] = set(query_processor.tokenize(text_to_tokenize))

    # Exemple de requête
    query = "web scraping produits"
    tokens = query_processor.tokenize(query)
    augmented_tokens = query_processor.augment_query(tokens)

    print("Requête originale:", query)
    print("Tokens:", tokens)
    print("Tokens augmentés:", augmented_tokens)

    # Filtrage des documents
    any_token_results = query_processor.filter_documents_any_token(documents, augmented_tokens)
    all_token_results = query_processor.filter_documents_all_tokens(documents, augmented_tokens)

    print(f"\nDocuments avec au moins un token: {len(any_token_results)}")
    print(f"Documents avec tous les tokens (hors stopwords): {len(all_token_results)}")

    # Afficher quelques détails des documents trouvés
    for doc in any_token_results[:3]:
        print("\nDocument trouvé:")
        print("URL:", doc.get('url'))
        print("Tokens du document:", doc['tokens'])

if __name__ == "__main__":
    main()

!pip install numpy
!pip install scikit-learn
!pip install nltk
!pip install rank-bm25
!pip install spacy
!python -m spacy download en_core_web_md

import json
import re
import unicodedata
import numpy as np
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi
from nltk.stem import PorterStemmer
import spacy

# Charger le modèle spaCy pour la similarité sémantique
nlp = spacy.load("en_core_web_md")

def adjust_weights(query):
      query_length = len(query.split())

      if query_length <= 2:  # Requête courte
          return {"TF-IDF": 0.15, "BM25": 0.4, "Exact Match": 0.3, "Semantic Embedding": 0.15}
      elif query_length <= 5:  # Requête moyenne
          return {"TF-IDF": 0.2, "BM25": 0.3, "Exact Match": 0.2, "Semantic Embedding": 0.3}
      else:  # Requête longue
          return {"TF-IDF": 0.25, "BM25": 0.2, "Exact Match": 0.15, "Semantic Embedding": 0.4}

class AdvancedSearchRanking:
    def __init__(self):
        self.documents = []
        self.tfidf_vectorizer = TfidfVectorizer()
        self.tfidf_matrix = None
        self.bm25 = None
        self.stemmer = PorterStemmer()

    def load_documents(self, jsonl_file):
        """Charger et préparer les documents"""
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            self.documents = [json.loads(line) for line in f]

        # Extraction du texte pour chaque document
        self.document_texts = [self._extract_text(doc) for doc in self.documents]

        # Préparation TF-IDF
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.document_texts)

        # Préparation BM25
        tokenized_corpus = [self._tokenize(doc) for doc in self.document_texts]
        self.bm25 = BM25Okapi(tokenized_corpus)

    def _extract_text(self, document):
        """Extraction de texte avec pondération"""
        parts = [
            document.get('title', '') * 3,  # Titre plus important
            document.get('description', '') * 2,  # Description importante
            str(document.get('product_features', '')),
            ' '.join(document.get('links', []))
        ]
        return ' '.join(parts)

    def _tokenize(self, text):
        """Tokenization avancée avec stemming"""
        text = unicodedata.normalize('NFKD', text.lower())
        text = text.encode('ascii', 'ignore').decode('utf-8')
        tokens = re.findall(r'\b\w+\b', text)
        return [self.stemmer.stem(token) for token in tokens]

    def semantic_similarity(self, query_tokens):
        """Calcul de similarité sémantique TF-IDF"""
        query_vector = self.tfidf_vectorizer.transform([' '.join(query_tokens)])
        similarities = np.dot(self.tfidf_matrix, query_vector.T).toarray().flatten()
        return similarities

    def bm25_score(self, query_tokens):
        """Score BM25"""
        return self.bm25.get_scores(query_tokens)

    def exact_match_score(self, query_tokens):
        """Score de correspondance exacte"""
        scores = []
        for doc_text in self.document_texts:
            doc_tokens = set(self._tokenize(doc_text))
            matching_tokens = len(set(query_tokens) & doc_tokens)
            score = matching_tokens / len(query_tokens) if query_tokens else 0
            scores.append(score)
        return scores

    def semantic_embedding_score(self, query):
        """Utilisation d'embeddings pour trouver les documents les plus proches en sens"""
        query_vector = nlp(query).vector
        scores = []

        for doc_text in self.document_texts:
            doc_vector = nlp(doc_text).vector
            score = np.dot(query_vector, doc_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(doc_vector))
            scores.append(score)

        return scores




    def rank_by_method(self, query):
        query_tokens = self._tokenize(query)

        scores_dict = {
            "TF-IDF": self.semantic_similarity(query_tokens),
            "BM25": self.bm25_score(query_tokens),
            "Exact Match": self.exact_match_score(query_tokens),
            "Semantic Embedding": self.semantic_embedding_score(query)
        }

        # Affichage des résultats par méthode
        for method, scores in scores_dict.items():
            print(f"\n--- Classement par {method} pour '{query}' ---")
            ranked_docs = sorted(zip(self.documents, scores), key=lambda x: x[1], reverse=True)

            for doc, score in ranked_docs[:3]:
                print(f"URL: {doc.get('url')}")
                print(f"Score: {score:.4f}")

        # Score global en combinant toutes les méthodes
        weights = adjust_weights(query)

        global_scores = np.zeros(len(self.documents))


        for method, scores in scores_dict.items():
            global_scores += np.array(scores) * weights[method]

        print(f"\n=== Classement Global pour '{query}' ===")
        ranked_docs = sorted(zip(self.documents, global_scores), key=lambda x: x[1], reverse=True)

        for doc, score in ranked_docs[:3]:
            print(f"URL: {doc.get('url')}")
            print(f"Score Global: {score:.4f}")

def main():
    ranking_engine = AdvancedSearchRanking()
    ranking_engine.load_documents('products.jsonl')

    # Exemples de requêtes
    queries = [
        "candy sugar",
        "boots",
        "hiking",
        "chocolat box",
        "sweat gift for a friend"
    ]

    for query in queries:
        ranking_engine.rank_by_method(query)

if __name__ == "__main__":
    main()

"""On utilise un système de poids dynamique.

En effet, les modèles "Exact Match" ainsi que "BM25" sont plutôt efficaces lorsque la requête a peu de mots. Au contraire, on va favoriser l'embedding et le TF-IDF qui permettent, eux, de capter plus facilement le contexte d'un ensemble de mots.

Ainsi, on va adapter les poids en fonction du nombre de mots présents dans la requête.

"""